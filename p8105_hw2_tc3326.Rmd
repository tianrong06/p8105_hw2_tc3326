---
title: "p8105_hw2_tc3326"
author: "KK Chen"
date: "2024-10-02"
output: github_document
---

```{r load_libraries}
library(tidyverse)
library(readxl)
```

# Problem 1
## Import and clean data
Read and clean the NYC Transit data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable

```{r}
cleaned_transit_df = 
  read_csv( "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",) |> 
  janitor::clean_names() |> 
  select("line", "station_name", "station_latitude", "station_longitude", 
    c("route1":"route11"), entry, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

transit_row = nrow(cleaned_transit_df)
transit_col = ncol(cleaned_transit_df)

transit_row
transit_col
```

## Write a short paragraph about this dataset:

* This dataset contains information about NYC subway station entrances and exits, including variables such as the subway line, station name, latitude, longitude, routes (route1 to route11), entry access, vending machine availability, entrance type, and ADA compliance. The data was cleaned by selecting relevant columns, standardizing column names using janitor::clean_names(), and converting the entry variable from "YES"/"NO" to a logical TRUE/FALSE. The cleaned dataset has 1,868 rows and 19 columns. 


## Answer the following questions using these data: 
```{r}
num_distinct_stations <- cleaned_transit_df |> 
  distinct(station_name, line) |> 
  nrow()

num_distinct_stations
```
* There are 465 distinct stations. 

```{r}
num_ada_compliant <- cleaned_transit_df |> 
  filter(ada == TRUE) |> 
  distinct(station_name, line) |> 
  nrow()

num_ada_compliant
```
* There are 84 stations are ADA compliant. 

```{r}
prop_no_vending<- cleaned_transit_df |> 
  filter(vending == "NO") |> 
  summarise(proportion = mean(entry)) |> 
  pull(proportion)

prop_no_vending
```
* There is 0.3770492 of station entrances/exits without vending allow entrance. 

```{r}
transit_long_df <- cleaned_transit_df |> 
  mutate(across(route1:route11, as.character)) |>  
  pivot_longer(
    cols = route1:route11, 
    names_to = "route_number", 
    values_to = "route"
  ) |> 
  filter(!is.na(route))  
```
```{r}
num_train_stations <- transit_long_df |> 
  filter(route == "A") |> 
  distinct(station_name, line) |> 
  nrow()

num_train_stations
```
*There are 60 distinct stations serve the A train

```{r}
num_adacompliant_train_stations <- transit_long_df |> 
  filter(route == "A", ada == TRUE) |> 
  distinct(station_name, line) |> 
  nrow()

num_adacompliant_train_stations
```
* There are 17 ADA compliant of the stations that serve the A train. 

# Problem 2
## Read and clean the Mr. Trash Wheel data
```{r}
trash_wheel_df = read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Mr. Trash Wheel",
  skip = 1,
  na = c("NA", ".", "") ) |> 
  janitor::clean_names() |>  
  filter(!is.na(dumpster)) |> 
 mutate(sports_balls = as.numeric(sports_balls)) |>  
 mutate(sports_balls = round(sports_balls, 0)) |>  
 mutate(sports_balls = as.integer(sports_balls), 
    trash_wheel = "Mr. Trash Wheel" 
 )
```

## Read and clean the Professor Trash Wheel data
```{r}
Professor_Trash_Wheel <- read_excel(
  path = "./data/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Professor Trash Wheel",
  na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |>   
  mutate(
    which_trashwheel = "Professor Trash Wheel",
    year = as.character(year)
  )
```

## Read and clean the Gwynnda Trash Wheel data
```{r}
Gwynnda_Trash_Wheel <- read_excel(
  path = "./data/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Gwynnda Trash Wheel",
  range = "A2:L265",  
  na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |>
  mutate(
    which_trashwheel = "Gwynnda Trash Wheel",
    year = as.character(year)
  )
```

## Combine Datasets to produce a single tidy dataset
```{r}
combined_trash_wheel_df <- bind_rows(trash_wheel_df, Professor_Trash_Wheel, Gwynnda_Trash_Wheel)
```

## Calculate number of observations and total weight of trash collected by Professor Trash Wheel and  total number of cigarette butts collected by Gwynnda in June of 2022.  
```{r}
nrow(combined_trash_wheel_df)

sum(combined_trash_wheel_df %>% filter(which_trashwheel == "Professor Trash Wheel") %>% pull(weight_tons), na.rm = TRUE)

sum(combined_trash_wheel_df %>% filter(which_trashwheel == "Gwynnda Trash Wheel", year == "2022", month == "June") %>% pull(cigarette_butts), na.rm = TRUE)
```

* The combined Trash Wheel dataset contains a total of 1033 observations from Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel dataset. This dataset includes key variables such as dumpster (the identifier for each dumpster load), weight_tons (the weight of trash collected in tons), volume_cubic_yards (the volume of trash collected), and cigarette_butts (the number of cigarette butts collected). For Professor Trash Wheel, the total weight of trash collected across all available data is 246.74 tons. Additionally, Gwynnda Trash Wheel collected a total of 18120 cigarette butts in June of 2022. This dataset provides valuable insights about the amount and type of trash collected by each Trash Wheel.

# Problem 3
## read and clean bakers data
```{r}
bakers_df <- read_csv(
  "./data/bakers.csv",
  na = c("NA", "N/A", "", ".")
) |>
  janitor::clean_names() |>
  mutate(baker_name = word(baker_name, 1)) |>
  rename(baker = baker_name)
```

## read and clean bakes data
```{r}
bakes_df <- read_csv(
  "./data/bakes.csv",
  na = c("NA", "N/A", "", ".")
) |>
  janitor::clean_names() |>
 mutate(
    baker = if_else(baker == '"Jo"', "Jo", baker)
    ) 
```

## read and clean results data
```{r}
results_df <- read_csv(
  "./data/results.csv",
  skip = 2,
  na = c("NA", "N/A", "", ".")
) |>
 janitor::clean_names()
```

## Check for bakers in `bakes_df` that are not present in `bakers_df`
```{r}
missing_in_bakers <- anti_join(bakes_df, bakers_df, by = c("baker","series") )
print(missing_in_bakers)
```

## Check for bakers in `results_df` that are not present in `bakers_df`
```{r}
missing_in_results <- anti_join(results_df, bakers_df, by = c("baker","series") )
print(missing_in_results)
```

## Merge `bakes_df` with `bakers_df` using `baker` and `series`
```{r}
combined_df <- results_df |> left_join(bakers_df, by = c("baker", "series"))
```

## Merge the resulting dataset with `results_df` using `baker`, `series`, and `episode`
```{r}
final_df <- combined_df |> left_join(bakes_df, by = c("baker", "series", "episode"))
```

## Export the result as a CSV
```{r}
write_csv(final_df, "./final_baking_dataset.csv")
```

## Describe your data cleaning process. Briefly discuss the final dataset.
* The data cleaning process involved importing and standardizing column names in the `bakers.csv`, `bakes.csv`, and `results.csv` datasets using `clean_names()`. In `bakers.csv`, we extracted only the first name from `baker_name` to facilitate merging. In `bakes.csv`, we corrected inconsistencies, such as removing quotation marks around "Jo", to ensure consistency in the baker column. We used `anti_join()` to identify mismatches between the datasets and resolved discrepancies before merging. The datasets were then merged using `baker`, `series`, and `episode` as keys to create a unified dataset, which was exported as `final_baking_dataset.csv`. 

* The final dataset `final_baking_dataset.csv` is a comprehensive collection of information on bakers, their bakes, and their performance across different series and episodes of the show. The key columns include `series`, `episode`, `baker`, `baker_age`, `baker_occupation`, `hometown`, `signature_bake`, `show_stoppe`, `technical`, and `result`. By merging data from `bakers.csv`, `bakes.csv`, and `results.csv`, the dataset captures each baker's journey and progression throughout the series. To enhance readability, columns were reorganized, and the data was sorted by `series`, `episode`, and `baker`. This structure allows for a detailed analysis of baking performance, trends, and outcomes within the competition.

## Create a reader-friendly table
```{r}
summary_table <- final_df %>%
  filter(series >= 5 & series <= 10) %>%
  filter(result == "STAR BAKER" | result == "WINNER") %>%
  arrange(series, episode, baker) %>%
  select(series, episode, baker, technical,result, baker_age)
```

* Winners generally ranked high in technicals. Star Bakers' technical appear to be less significant.
* Most winners in the table had previously been named Star Bakers at least once. However, an exception is David who is the winner at the end without ever being named Star Baker before. 

## Import, clean, tidy, and organize the viewership data in viewers.csv. 
```{r}
viewers_df = read_csv("./data/viewers.csv") %>% 
  janitor::clean_names()%>%
  head(10)
```

## the average viewership in Season 1 and In Season 5
```{r}
average_viewership_1 = viewers_df %>% 
  pull(series_1)
mean(average_viewership_1, na.rm = TRUE)
```

```{r}
average_viewership_5 = viewers_df %>% 
  pull(series_5)
mean(average_viewership_5, na.rm = TRUE)
```

* The average viewership in Season 1 was 2.77, in Season 5 was 10.0393.














